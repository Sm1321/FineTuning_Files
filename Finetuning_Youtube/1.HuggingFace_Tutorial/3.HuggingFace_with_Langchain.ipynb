{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3af1f4f1",
   "metadata": {},
   "source": [
    "## Huggingface with Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6444be",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/integrations/providers/huggingface/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9743940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate\n",
    "# !pip install  bitsandbytes\n",
    "# # !pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f9aacae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-huggingface\n",
    "# !pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fbad141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f81b40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b537df65",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id = \"deepseek-ai/DeepSeek-R1\",\n",
    "    task  =\"text-generation\",\n",
    "    max_new_tokens = 512,\n",
    "    do_sample = False,\n",
    "    repetition_penalty = 1.03,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e40702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatHuggingFace(llm = llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7834a80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, so I need to figure out how to respond to \"hello how are you?\" as an AI assistant. Let me start by breaking down the components of the query. The user greeted me with \"hello\" and then asked about my well-being. Since I\\'m an AI, I don\\'t have feelings, but I should respond politely.\\n\\nFirst, I should acknowledge the greeting. \"Hello!\" seems like a good start. Then, addressing the question about how I am. I need to mention that I don\\'t have feelings but maybe express willingness to help. Maybe something like \"I\\'m just a program, so I don\\'t have feelings, but I\\'m here and ready to help you with whatever you need!\" Then, to keep the conversation going, I can ask how they are doing. But wait, maybe the user doesn\\'t want the conversation to drag on. Hmm, should I offer help directly instead? Let me see examples. Sometimes, responses include thanking the user and prompting them to ask for assistance. Oh, right, so after the initial response, I should transition into offering help. So maybe, \"How can I assist you today?\" That would be a good way to end the response. Let me put it all together: \"Hello! I\\'m just a program, so I don\\'t have feelings, but I\\'m here and ready to help you with whatever you need. How can I assist you today?\" Does that flow well? Let me check for any errors or awkward phrasing. No, it seems okay. Alternative phrasing could be \"I\\'m an AI and don\\'t experience emotions, but I\\'m always here to help! What can I do for you today?\" But maybe keeping it simpler is better. Okay, I think that\\'s a good response.\\n</think>\\n\\n\\n\\nHello! Since I\\'m an AI, I don\\'t have feelings, but I\\'m here and ready to assist you with anything you need. How can I help you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 396, 'prompt_tokens': 10, 'total_tokens': 406}, 'model_name': 'deepseek-ai/DeepSeek-R1', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None}, id='run--b82ceb0b-6afa-4cb9-a2d3-970f9988804c-0', usage_metadata={'input_tokens': 10, 'output_tokens': 396, 'total_tokens': 406})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(\"hello how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28502597",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4ff54bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "give me a answer in detail manner and in step by step manner\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template = template,input_variables = [\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d115197",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = (\n",
    "        {\"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | chat_model\n",
    "        | StrOutputParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45a453e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"who is a first president of INDIA?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cad98d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f43710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_huggingface import ChatHuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7627928",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs=dict(\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        repetition_penalty=1.03,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23697edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec202b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs=dict(\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        repetition_penalty=1.03,\n",
    "        return_full_text=False,\n",
    "    ),\n",
    "    model_kwargs={\"quantization_config\": quantization_config},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2245bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dbbbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template=\"\"\"Question: {question}\n",
    "give me a answer in detail manner and in step by step manner\"\"\"\n",
    "\n",
    "prompt=PromptTemplate(template=template,input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051cc4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = (\n",
    "        {\"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | chat_model\n",
    "        | StrOutputParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00bd3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"who is a first president of INDIA?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921de6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24744b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# from langchain_huggingface import HuggingFaceEndpointEmbeddings\n",
    "# from langchain_community.document_loaders.hugging_face_dataset import HuggingFaceDatasetLoader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
