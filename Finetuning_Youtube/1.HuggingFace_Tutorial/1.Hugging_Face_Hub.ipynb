{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cec2d307",
   "metadata": {},
   "source": [
    "## Huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c20c5b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade \"huggingface_hub>=0.33.1\" -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c90a414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5513b841f2b841e18533fba822f51018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()  # Will prompt for your token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4056342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b440c54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model info without downloading it\n",
    "model_info = api.model_info(repo_id=\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a45d5475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Card Metadata: datasets:\n",
      "- bookcorpus\n",
      "- wikipedia\n",
      "language: en\n",
      "license: apache-2.0\n",
      "tags:\n",
      "- exbert\n",
      "Tags: ['transformers', 'pytorch', 'tf', 'jax', 'rust', 'coreml', 'onnx', 'safetensors', 'bert', 'fill-mask', 'exbert', 'en', 'dataset:bookcorpus', 'dataset:wikipedia', 'arxiv:1810.04805', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us']\n",
      "Commit SHA: 86b5e0934494bd15c9632b12f734a8a67f723594\n",
      "Last Modified: 2024-02-19 11:06:12+00:00\n",
      "Files: ['.gitattributes', 'LICENSE', 'README.md', 'config.json', 'coreml/fill-mask/float32_model.mlpackage/Data/com.apple.CoreML/model.mlmodel', 'coreml/fill-mask/float32_model.mlpackage/Data/com.apple.CoreML/weights/weight.bin', 'coreml/fill-mask/float32_model.mlpackage/Manifest.json', 'flax_model.msgpack', 'model.onnx', 'model.safetensors', 'pytorch_model.bin', 'rust_model.ot', 'tf_model.h5', 'tokenizer.json', 'tokenizer_config.json', 'vocab.txt']\n",
      "Model ID: Not available\n",
      "Likes: 2357\n",
      "Author/Namespace: google-bert\n",
      "Is Private: False\n",
      "Downloads: 55961221\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Card Metadata:\", model_info.card_data)\n",
    "print(\"Tags:\", model_info.tags)\n",
    "print(\"Commit SHA:\", model_info.sha)\n",
    "print(\"Last Modified:\", model_info.last_modified)\n",
    "print(\"Files:\", [f.rfilename for f in model_info.siblings])\n",
    "print(\"Model ID:\", model_info.card_data.get('modelId', 'Not available'))\n",
    "print(\"Likes:\", model_info.likes)\n",
    "print(\"Author/Namespace:\", model_info.author)\n",
    "print(\"Is Private:\", model_info.private)\n",
    "print(\"Downloads:\", model_info.downloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5589729",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = api.list_models(search=\"emotion\", sort=\"downloads\", limit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0092b00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain/emotion-recognition-wav2vec2-IEMOCAP - 839719\n",
      "j-hartmann/emotion-english-distilroberta-base - 532411\n",
      "audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim - 373708\n",
      "SamLowe/roberta-base-go_emotions - 356893\n",
      "Djacon/rubert-tiny2-russian-emotion-detection - 227522\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(model.modelId, \"-\", model.downloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e473ce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = api.list_models(filter=\"text-generation\", sort=\"downloads\", limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05488fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai-community/gpt2 - 14347531\n",
      "Qwen/Qwen2.5-14B-Instruct - 12155604\n",
      "meta-llama/Llama-3.1-8B-Instruct - 10001312\n",
      "Qwen/Qwen2.5-7B-Instruct - 7927159\n",
      "Qwen/Qwen3-4B-Base - 7520523\n",
      "facebook/opt-125m - 5335329\n",
      "dphn/dolphin-2.9.1-yi-1.5-34b - 4706625\n",
      "Qwen/Qwen3-8B-Base - 4277209\n",
      "Qwen/Qwen3-0.6B - 4103202\n",
      "Gensyn/Qwen2.5-0.5B-Instruct - 3837425\n"
     ]
    }
   ],
   "source": [
    "for m in models:\n",
    "    print(m.modelId, \"-\", m.downloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb69b2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import list_repo_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc0a348b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.gitattributes', 'README.md', 'config.json', 'flax_model.msgpack', 'generation_config.json', 'model.safetensors', 'pytorch_model.bin', 'special_tokens_map.json', 'spiece.model', 'tf_model.h5', 'tokenizer.json', 'tokenizer_config.json']\n"
     ]
    }
   ],
   "source": [
    "files = list_repo_files(\"google/flan-t5-base\")\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "caa97e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = api.dataset_info(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8b9391b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description: No summary\n",
      "Files: ['.gitattributes', 'README.md', 'plain_text/test-00000-of-00001.parquet', 'plain_text/train-00000-of-00001.parquet', 'plain_text/unsupervised-00000-of-00001.parquet']\n"
     ]
    }
   ],
   "source": [
    "print(\"Description:\", dataset.cardData.get(\"summary\", \"No summary\"))\n",
    "print(\"Files:\", [f.rfilename for f in dataset.siblings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8582b003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden size: 768\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "# Download config.json\n",
    "config_path = hf_hub_download(repo_id=\"bert-base-uncased\", filename=\"config.json\")\n",
    "with open(config_path) as f:\n",
    "    config = json.load(f)\n",
    "print(\"Hidden size:\", config[\"hidden_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d8a26b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextClassificationOutputElement(label='POSITIVE', score=0.9998641014099121), TextClassificationOutputElement(label='NEGATIVE', score=0.0001358972367597744)]\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# Use the correct hosted model ID with repository owner prefix\n",
    "client = InferenceClient(model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "response = client.text_classification(\"I love Hugging Face! \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42ad1c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline tag: text-classification\n"
     ]
    }
   ],
   "source": [
    "info = api.model_info(\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "print(\"Pipeline tag:\", info.pipeline_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "567bd781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample models with inference support:\n",
      "- zai-org/GLM-4.5\n",
      "- Chain-GPT/Solidity-LLM\n",
      "- Qwen/Qwen3-Coder-480B-A35B-Instruct\n",
      "- Qwen/Qwen3-30B-A3B-Instruct-2507\n",
      "- Qwen/Qwen3-235B-A22B-Thinking-2507\n",
      "- zai-org/GLM-4.5-Air\n",
      "- Qwen/Qwen3-Coder-30B-A3B-Instruct\n",
      "- moonshotai/Kimi-K2-Instruct\n",
      "- Qwen/Qwen3-30B-A3B-Thinking-2507\n",
      "- nvidia/Llama-3_3-Nemotron-Super-49B-v1_5\n",
      "- Tesslate/UIGEN-X-32B-0727\n",
      "- Qwen/Qwen3-235B-A22B-Instruct-2507\n",
      "- stepfun-ai/step3\n",
      "- PowerInfer/SmallThinker-21BA3B-Instruct\n",
      "- unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF\n",
      "- zai-org/GLM-4.5-FP8\n",
      "- OmniSVG/OmniSVG\n",
      "- Qwen/Qwen3-30B-A3B-Instruct-2507-FP8\n",
      "- arcee-ai/AFM-4.5B\n",
      "- zai-org/GLM-4.5-Base\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "# Search text generation models with hosted inference support\n",
    "models = api.list_models(filter=\"text-generation\", limit=20)\n",
    "print(\"Sample models with inference support:\")\n",
    "for m in models:\n",
    "    if m.pipeline_tag == \"text-generation\":\n",
    "        print(\"-\", m.modelId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a39a1358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 models\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Pull only \"warm\" text-generation models served by HF-Inference, max 25 results\n",
    "gen = api.list_models(\n",
    "        inference_provider=\"hf-inference\",\n",
    "        pipeline_tag=\"text-generation\",\n",
    "        limit=25)\n",
    "\n",
    "models = list(gen)\n",
    "print(f\"Found {len(models)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8391455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModelInfo(id='HuggingFaceTB/SmolLM3-3B', author=None, sha=None, created_at=datetime.datetime(2025, 7, 8, 10, 11, 45, tzinfo=datetime.timezone.utc), last_modified=None, private=False, disabled=None, downloads=665723, downloads_all_time=None, gated=None, gguf=None, inference=None, inference_provider_mapping=None, likes=618, library_name='transformers', tags=['transformers', 'safetensors', 'smollm3', 'text-generation', 'conversational', 'en', 'fr', 'es', 'it', 'pt', 'zh', 'ar', 'ru', 'base_model:HuggingFaceTB/SmolLM3-3B-Base', 'base_model:finetune:HuggingFaceTB/SmolLM3-3B-Base', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us'], pipeline_tag='text-generation', mask_token=None, card_data=None, widget_data=None, model_index=None, config=None, transformers_info=None, trending_score=32, siblings=None, spaces=None, safetensors=None, security_repo_status=None, xet_enabled=None)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "30b8217e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFaceTB/SmolLM3-3B\n"
     ]
    }
   ],
   "source": [
    "for model_name in models:\n",
    "      print(model_name.modelId)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f00cad1",
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "404 Client Error: Not Found for url: https://router.huggingface.co/hf-inference/models/meta-llama/Meta-Llama-3-8B-Instruct (Request ID: Root=1-688c8f9b-395c1d03157d6d296bd7e6cd;2f1bd4e3-a379-4714-b8cc-5c745cea5967)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\MY_Folder\\Git_FineTuning_Folder\\FineTuning_Files\\finetune_env\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\MY_Folder\\Git_FineTuning_Folder\\FineTuning_Files\\finetune_env\\lib\\site-packages\\requests\\models.py:1026\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://router.huggingface.co/hf-inference/models/meta-llama/Meta-Llama-3-8B-Instruct",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InferenceClient\n\u001b[0;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m InferenceClient(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m, provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf-inference\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI saw a puppy, a cat and a raccoon during my bike ride in the park.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\MY_Folder\\Git_FineTuning_Folder\\FineTuning_Files\\finetune_env\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:2337\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[1;34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[0m\n\u001b[0;32m   2312\u001b[0m         _set_unsupported_text_generation_kwargs(model, unused_params)\n\u001b[0;32m   2313\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_generation(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   2314\u001b[0m             prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m   2315\u001b[0m             details\u001b[38;5;241m=\u001b[39mdetails,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2335\u001b[0m             watermark\u001b[38;5;241m=\u001b[39mwatermark,\n\u001b[0;32m   2336\u001b[0m         )\n\u001b[1;32m-> 2337\u001b[0m     \u001b[43mraise_text_generation_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2339\u001b[0m \u001b[38;5;66;03m# Parse output\u001b[39;00m\n\u001b[0;32m   2340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "File \u001b[1;32mc:\\MY_Folder\\Git_FineTuning_Folder\\FineTuning_Files\\finetune_env\\lib\\site-packages\\huggingface_hub\\inference\\_common.py:437\u001b[0m, in \u001b[0;36mraise_text_generation_error\u001b[1;34m(http_error)\u001b[0m\n\u001b[0;32m    435\u001b[0m     error_type \u001b[38;5;241m=\u001b[39m payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror_type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:  \u001b[38;5;66;03m# no payload\u001b[39;00m\n\u001b[1;32m--> 437\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m http_error\n\u001b[0;32m    439\u001b[0m \u001b[38;5;66;03m# If error_type => more information than `hf_raise_for_status`\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\MY_Folder\\Git_FineTuning_Folder\\FineTuning_Files\\finetune_env\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:2307\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[1;34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[0m\n\u001b[0;32m   2305\u001b[0m \u001b[38;5;66;03m# Handle errors separately for more precise error messages\u001b[39;00m\n\u001b[0;32m   2306\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2307\u001b[0m     bytes_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   2308\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2309\u001b[0m     match \u001b[38;5;241m=\u001b[39m MODEL_KWARGS_NOT_USED_REGEX\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[1;32mc:\\MY_Folder\\Git_FineTuning_Folder\\FineTuning_Files\\finetune_env\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:278\u001b[0m, in \u001b[0;36mInferenceClient._inner_post\u001b[1;34m(self, request_parameters, stream)\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 278\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32mc:\\MY_Folder\\Git_FineTuning_Folder\\FineTuning_Files\\finetune_env\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:482\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://router.huggingface.co/hf-inference/models/meta-llama/Meta-Llama-3-8B-Instruct (Request ID: Root=1-688c8f9b-395c1d03157d6d296bd7e6cd;2f1bd4e3-a379-4714-b8cc-5c745cea5967)"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(model=\"meta-llama/Meta-Llama-3-8B-Instruct\", provider=\"hf-inference\")\n",
    "\n",
    "output = client.text_generation(\n",
    "    prompt=\"I saw a puppy, a cat and a raccoon during my bike ride in the park.\",\n",
    "    max_new_tokens=80,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb320bb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Model meta-llama/Meta-Llama-3-8B-Instruct is not supported for task text-generation and provider novita. Supported task: conversational.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m InferenceClient(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Generate text\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI saw a puppy, a cat and a raccoon during my bike ride in the park.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\MY_Folder\\Git_FineTuning_Folder\\FineTuning_Files\\finetune_env\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:2296\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[1;34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[0m\n\u001b[0;32m   2294\u001b[0m model_id \u001b[38;5;241m=\u001b[39m model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   2295\u001b[0m provider_helper \u001b[38;5;241m=\u001b[39m get_provider_helper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovider, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel_id)\n\u001b[1;32m-> 2296\u001b[0m request_parameters \u001b[38;5;241m=\u001b[39m \u001b[43mprovider_helper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2297\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_payload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2303\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2305\u001b[0m \u001b[38;5;66;03m# Handle errors separately for more precise error messages\u001b[39;00m\n\u001b[0;32m   2306\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\MY_Folder\\Git_FineTuning_Folder\\FineTuning_Files\\finetune_env\\lib\\site-packages\\huggingface_hub\\inference\\_providers\\_common.py:90\u001b[0m, in \u001b[0;36mTaskProviderHelper.prepare_request\u001b[1;34m(self, inputs, parameters, headers, model, api_key, extra_payload)\u001b[0m\n\u001b[0;32m     87\u001b[0m api_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_api_key(api_key)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# mapped model from HF model ID\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m provider_mapping_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_mapping_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# default HF headers + user headers (to customize in subclasses)\u001b[39;00m\n\u001b[0;32m     93\u001b[0m headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_headers(headers, api_key)\n",
      "File \u001b[1;32mc:\\MY_Folder\\Git_FineTuning_Folder\\FineTuning_Files\\finetune_env\\lib\\site-packages\\huggingface_hub\\inference\\_providers\\_common.py:159\u001b[0m, in \u001b[0;36mTaskProviderHelper._prepare_mapping_info\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported by provider \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m provider_mapping\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask:\n\u001b[1;32m--> 159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported for task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and provider \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupported task: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprovider_mapping\u001b[38;5;241m.\u001b[39mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    162\u001b[0m     )\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m provider_mapping\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaging\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    165\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    166\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is in staging mode for provider \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Meant for test purposes only.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    167\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Model meta-llama/Meta-Llama-3-8B-Instruct is not supported for task text-generation and provider novita. Supported task: conversational."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(model=\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "\n",
    "# Generate text\n",
    "output = client.text_generation(\n",
    "    prompt=\"I saw a puppy, a cat and a raccoon during my bike ride in the park.\",\n",
    "    max_new_tokens=80,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ea2e62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
