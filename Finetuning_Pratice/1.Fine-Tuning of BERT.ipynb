{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN+C+AWqE+rvnSW/YNnzi/4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"58708e6cd79a40bb809e45202d7d9998":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_d7d1b96c1fa4434697a063008d4c06e6","IPY_MODEL_064c709db1064bc1881904942bce1c8f","IPY_MODEL_595cae5936284202b9de45a38a107cc3","IPY_MODEL_9fa90c6742c34141ac402e821fada85f","IPY_MODEL_f069894f61b741e2a5c6931d8fd1a0e1"],"layout":"IPY_MODEL_401c3dd0eacf4427afa875b8c20bfc44"}},"d7d1b96c1fa4434697a063008d4c06e6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5fb0cf0f440d47dbac5932d35cbf11ad","placeholder":"â€‹","style":"IPY_MODEL_5f3aff67815c438d91961458b173bf77","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"064c709db1064bc1881904942bce1c8f":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_f92207955edb426193834238fd3126f8","placeholder":"â€‹","style":"IPY_MODEL_9f29b91a2bcf4dd7b1112446505c280d","value":""}},"595cae5936284202b9de45a38a107cc3":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_1f4364049f054414b090c73107db6d1f","style":"IPY_MODEL_d1bbbbf652434e2a8440338b5c759334","value":true}},"9fa90c6742c34141ac402e821fada85f":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_631f8aaba1d6440391e476b6a646f597","style":"IPY_MODEL_87da451d48974bfb9e5b7a277568b8e9","tooltip":""}},"f069894f61b741e2a5c6931d8fd1a0e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f15beccd7d14875b682aa6bd2905a0b","placeholder":"â€‹","style":"IPY_MODEL_99a48b8ce90b4c58907c9dc8609b61d0","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"401c3dd0eacf4427afa875b8c20bfc44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"5fb0cf0f440d47dbac5932d35cbf11ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f3aff67815c438d91961458b173bf77":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f92207955edb426193834238fd3126f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f29b91a2bcf4dd7b1112446505c280d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1f4364049f054414b090c73107db6d1f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1bbbbf652434e2a8440338b5c759334":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"631f8aaba1d6440391e476b6a646f597":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87da451d48974bfb9e5b7a277568b8e9":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"2f15beccd7d14875b682aa6bd2905a0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99a48b8ce90b4c58907c9dc8609b61d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"79d892335c054002a70fad17df0a5cfc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8759eff675d24092a3239b6b7575844c","IPY_MODEL_c7847df5323d4bc9ac0a7efdc7cbcf11","IPY_MODEL_e8c96e0658fd4f4ab4c5429ed7c342fc"],"layout":"IPY_MODEL_2ada59cfd10744679846eb9c994d0469"}},"8759eff675d24092a3239b6b7575844c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d8b08e83e51463c8fb9469a49c7b814","placeholder":"â€‹","style":"IPY_MODEL_022f0ef666db4d149df6651744ca4cd0","value":"Map:â€‡100%"}},"c7847df5323d4bc9ac0a7efdc7cbcf11":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3400bdad6845400794b36c04535c7754","max":50000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0232e3f53a3e40459d5c282b439c06c5","value":50000}},"e8c96e0658fd4f4ab4c5429ed7c342fc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e7a937f0a6242e594e9678b0465f583","placeholder":"â€‹","style":"IPY_MODEL_b5aec56d03ef4e2d87ee5ae97b5e7dad","value":"â€‡50000/50000â€‡[01:50&lt;00:00,â€‡680.50â€‡examples/s]"}},"2ada59cfd10744679846eb9c994d0469":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d8b08e83e51463c8fb9469a49c7b814":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"022f0ef666db4d149df6651744ca4cd0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3400bdad6845400794b36c04535c7754":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0232e3f53a3e40459d5c282b439c06c5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3e7a937f0a6242e594e9678b0465f583":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5aec56d03ef4e2d87ee5ae97b5e7dad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["### Step-by-Step Guide to Fine-Tuning a BERT Model for Text Classification\n","- We will fine-tune a BERT model (Bidirectional Encoder Representations from Transformers) for a binary sentiment classification task (positive/negative reviews) using the IMDB movie reviews dataset."],"metadata":{"id":"L_LvPYJSPdhZ"}},{"cell_type":"markdown","source":["### Step 1: Install Required Libraries\n","- First, install the necessary Python libraries:"],"metadata":{"id":"Oul4TAHuO6jX"}},{"cell_type":"markdown","source":["- `transformers` â†’ Contains pre-trained models like BERT.\n","\n","- `datasets` â†’ Provides easy access to datasets (e.g., IMDB dataset).\n","\n","- `torch` â†’ PyTorch, required for model training.\n","\n","- `sklearn` â†’ Used for evaluation metrics."],"metadata":{"id":"61X1BpYEPm9D"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"kqlQ-Z9ZOjY7","executionInfo":{"status":"ok","timestamp":1743159073337,"user_tz":-330,"elapsed":7896,"user":{"displayName":"Mohan DS","userId":"05181729263002066506"}}},"outputs":[],"source":["! pip install transformers datasets torch -q"]},{"cell_type":"code","source":["import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\""],"metadata":{"id":"2RdM3f2BT6ek","executionInfo":{"status":"ok","timestamp":1743159073346,"user_tz":-330,"elapsed":5,"user":{"displayName":"Mohan DS","userId":"05181729263002066506"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### Step 2: Import Necessary Libraries\n"],"metadata":{"id":"vxzMSaQyPwWy"}},{"cell_type":"code","source":["# Hugging Face Libraries\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n","from datasets import load_dataset\n","from huggingface_hub import login\n","import torch\n","\n","# Model Evaluation Libraries\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","\n","# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rekZPGqMPzUD","executionInfo":{"status":"ok","timestamp":1743159113247,"user_tz":-330,"elapsed":39897,"user":{"displayName":"Mohan DS","userId":"05181729263002066506"}},"outputId":"f76bee3a-2020-4cf3-b9ed-e267a133c1fe"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]}]},{"cell_type":"markdown","source":["- `AutoModelForSequenceClassification` â†’ Loads a BERT model for text classification.\n","\n","- `AutoTokenizer` â†’ Converts text into a format suitable for BERT.\n","\n","- `Trainer & TrainingArguments` â†’ Simplifies model training.\n","\n","- `load_dataset` â†’ Loads datasets like IMDB easily.\n","\n","- `torch.device` â†’ Moves the model to GPU (if available) for faster training."],"metadata":{"id":"FEqxLzjLP2SO"}},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":331,"referenced_widgets":["58708e6cd79a40bb809e45202d7d9998","d7d1b96c1fa4434697a063008d4c06e6","064c709db1064bc1881904942bce1c8f","595cae5936284202b9de45a38a107cc3","9fa90c6742c34141ac402e821fada85f","f069894f61b741e2a5c6931d8fd1a0e1","401c3dd0eacf4427afa875b8c20bfc44","5fb0cf0f440d47dbac5932d35cbf11ad","5f3aff67815c438d91961458b173bf77","f92207955edb426193834238fd3126f8","9f29b91a2bcf4dd7b1112446505c280d","1f4364049f054414b090c73107db6d1f","d1bbbbf652434e2a8440338b5c759334","631f8aaba1d6440391e476b6a646f597","87da451d48974bfb9e5b7a277568b8e9","2f15beccd7d14875b682aa6bd2905a0b","99a48b8ce90b4c58907c9dc8609b61d0"]},"id":"hjVKUZd1TM0m","executionInfo":{"status":"ok","timestamp":1743159113263,"user_tz":-330,"elapsed":22,"user":{"displayName":"Mohan DS","userId":"05181729263002066506"}},"outputId":"e14eeed0-de7f-49f0-b2dc-cf33ae83b753"},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58708e6cd79a40bb809e45202d7d9998"}},"metadata":{}}]},{"cell_type":"markdown","source":["### Step 3: Load Pre-trained BERT Model and Tokenizer\n","- We will use bert-base-uncased, a version of BERT trained on uncased English text."],"metadata":{"id":"Tx0MD81RO_o_"}},{"cell_type":"code","source":["# Load a pre-trained BERT model for text classification\n","model_name = \"bert-base-uncased\"\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Binary classification\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NzzYDM06O3i9","executionInfo":{"status":"ok","timestamp":1743159114897,"user_tz":-330,"elapsed":1633,"user":{"displayName":"Mohan DS","userId":"05181729263002066506"}},"outputId":"904514a0-1e0e-4894-b8f7-54dcd8277f3a"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"markdown","source":["- `bert-base-uncased` â†’ A BERT model trained on lowercased English text.\n","\n","- `num_labels=2` â†’ Since we are doing binary classification (positive/negative), we set 2 output classes."],"metadata":{"id":"EOWl8MfFQL7c"}},{"cell_type":"code","source":["# Move the model to GPU if available\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BLq6WJSxO3lh","executionInfo":{"status":"ok","timestamp":1743159114920,"user_tz":-330,"elapsed":24,"user":{"displayName":"Mohan DS","userId":"05181729263002066506"}},"outputId":"02a60337-0547-480e-916f-f99c572df28d"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSdpaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["### Step 4: Load and Preprocess the Dataset"],"metadata":{"id":"cfJSFCPnQYRx"}},{"cell_type":"code","source":["# Load the IMDB dataset\n","dataset = load_dataset(\"imdb\")\n","\n","# Tokenize the dataset\n","def preprocess_function(examples):\n","    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n","\n","# Apply the tokenization function to the dataset\n","encoded_dataset = dataset.map(preprocess_function, batched=True)\n","\n","# Split dataset into train and test sets\n","train_dataset = encoded_dataset[\"train\"]\n","test_dataset = encoded_dataset[\"test\"]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["79d892335c054002a70fad17df0a5cfc","8759eff675d24092a3239b6b7575844c","c7847df5323d4bc9ac0a7efdc7cbcf11","e8c96e0658fd4f4ab4c5429ed7c342fc","2ada59cfd10744679846eb9c994d0469","8d8b08e83e51463c8fb9469a49c7b814","022f0ef666db4d149df6651744ca4cd0","3400bdad6845400794b36c04535c7754","0232e3f53a3e40459d5c282b439c06c5","3e7a937f0a6242e594e9678b0465f583","b5aec56d03ef4e2d87ee5ae97b5e7dad"]},"id":"ew11xASOO3of","executionInfo":{"status":"ok","timestamp":1743159229596,"user_tz":-330,"elapsed":114675,"user":{"displayName":"Mohan DS","userId":"05181729263002066506"}},"outputId":"d87b4711-4157-4ac6-f5ce-af0624988848"},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79d892335c054002a70fad17df0a5cfc"}},"metadata":{}}]},{"cell_type":"markdown","source":["- `load_dataset(\"imdb\")` â†’ Downloads the IMDB movie reviews dataset.\n","\n","- `tokenizer()` â†’ Converts text into token IDs that BERT understands.\n","\n","- `truncation=True` â†’ Ensures the text length does not exceed 512 tokens (BERTâ€™s limit).\n","\n","- `padding=\"max_length\"` â†’ Ensures each sequence has the same length for batch training.\n","\n","- `dataset.map()` â†’ Applies tokenization to every text sample in the dataset."],"metadata":{"id":"4ecmcGeiQc6O"}},{"cell_type":"code","source":["dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dsj6fmbjQbYH","executionInfo":{"status":"ok","timestamp":1743159229622,"user_tz":-330,"elapsed":28,"user":{"displayName":"Mohan DS","userId":"05181729263002066506"}},"outputId":"992adcb8-1e2e-4e1c-87e4-aa5eee0b4323"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 25000\n","    })\n","    test: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 25000\n","    })\n","    unsupervised: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 50000\n","    })\n","})"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["dataset.column_names"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ylsUPHdGR3Yr","executionInfo":{"status":"ok","timestamp":1743159229628,"user_tz":-330,"elapsed":8,"user":{"displayName":"Mohan DS","userId":"05181729263002066506"}},"outputId":"439a9f2b-e61d-42e7-e1e1-924de85b1aea"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'train': ['text', 'label'],\n"," 'test': ['text', 'label'],\n"," 'unsupervised': ['text', 'label']}"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["train_dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2id439BARNdC","executionInfo":{"status":"ok","timestamp":1743159229657,"user_tz":-330,"elapsed":28,"user":{"displayName":"Mohan DS","userId":"05181729263002066506"}},"outputId":"0a4aa44c-b42b-4890-f121-3c0e68bb9aa7"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n","    num_rows: 25000\n","})"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["print(dataset[\"train\"].column_names)\n","print(dataset[\"train\"][0])  # Print first example\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o5lapaYjSE6m","executionInfo":{"status":"ok","timestamp":1743159229668,"user_tz":-330,"elapsed":8,"user":{"displayName":"Mohan DS","userId":"05181729263002066506"}},"outputId":"09ad86f6-f5db-4625-f5e6-a7d1771b8eac"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["['text', 'label']\n","{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n"]}]},{"cell_type":"code","source":["print(dataset[\"test\"].column_names)\n","print(dataset[\"test\"][0])  # Print first example\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yw1vYxoHRRLx","executionInfo":{"status":"ok","timestamp":1743159229703,"user_tz":-330,"elapsed":34,"user":{"displayName":"Mohan DS","userId":"05181729263002066506"}},"outputId":"300dfd02-5d4a-453f-88db-0fd6c9e369cf"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["['text', 'label']\n","{'text': 'I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clichÃ©d and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.', 'label': 0}\n"]}]},{"cell_type":"code","source":["print(dataset[\"unsupervised\"].column_names)\n","print(dataset[\"unsupervised\"][0])  # Print first example\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"woN50aVgSbB3","executionInfo":{"status":"ok","timestamp":1743159229704,"user_tz":-330,"elapsed":6,"user":{"displayName":"Mohan DS","userId":"05181729263002066506"}},"outputId":"fd753406-29b4-4505-874b-e19a3a681649"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["['text', 'label']\n","{'text': 'This is just a precious little diamond. The play, the script are excellent. I cant compare this movie with anything else, maybe except the movie \"Leon\" wonderfully played by Jean Reno and Natalie Portman. But... What can I say about this one? This is the best movie Anne Parillaud has ever played in (See please \"Frankie Starlight\", she\\'s speaking English there) to see what I mean. The story of young punk girl Nikita, taken into the depraved world of the secret government forces has been exceptionally over used by Americans. Never mind the \"Point of no return\" and especially the \"La femme Nikita\" TV series. They cannot compare the original believe me! Trash these videos. Buy this one, do not rent it, BUY it. BTW beware of the subtitles of the LA company which \"translate\" the US release. What a disgrace! If you cant understand French, get a dubbed version. But you\\'ll regret later :)', 'label': -1}\n"]}]},{"cell_type":"code","source":["train_dataset.column_names"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"il_6BG7gSgUq","executionInfo":{"status":"ok","timestamp":1743159229711,"user_tz":-330,"elapsed":9,"user":{"displayName":"Mohan DS","userId":"05181729263002066506"}},"outputId":"4b63e33f-44f5-47a7-d6b9-2b3e5580af06"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask']"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["test_dataset.column_names"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X2uaAho_RPXO","executionInfo":{"status":"ok","timestamp":1743159229718,"user_tz":-330,"elapsed":6,"user":{"displayName":"Mohan DS","userId":"05181729263002066506"}},"outputId":"7cbefebe-687e-483d-970b-4dd4a17a0e17"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask']"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["### Step 5: Define Evaluation Metrics\n"],"metadata":{"id":"cIWfc5xIQ57W"}},{"cell_type":"code","source":["\n","# Define a function to compute accuracy, precision, recall, and F1 score\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"binary\")\n","    acc = accuracy_score(labels, predictions)\n","    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n"],"metadata":{"id":"-UOuei8jQ3GM","executionInfo":{"status":"ok","timestamp":1743159229730,"user_tz":-330,"elapsed":8,"user":{"displayName":"Mohan DS","userId":"05181729263002066506"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["- `accuracy_score` â†’ Measures how many predictions are correct.\n","\n","- `precision_recall_fscore_support` â†’ Computes precision, recall, and F1-score.\n","\n","- `argmax(logits)` â†’ Converts model outputs into class predictions (0 or 1)."],"metadata":{"id":"Nzb4kgzcRATS"}},{"cell_type":"markdown","source":["### Step 6: Define Training Arguments\n"],"metadata":{"id":"aIEf8F26RJEn"}},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    output_dir=\"/content/BERT-IMDB\",  # Where to save the model\n","    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n","    save_strategy=\"epoch\",  # Save model checkpoints\n","    learning_rate=2e-5,  # Standard learning rate for fine-tuning BERT\n","    per_device_train_batch_size=2,  # Batch size for training\n","    per_device_eval_batch_size=2,  # Batch size for evaluation\n","    num_train_epochs=1,  # Train for 3 epochs\n","    weight_decay=0.01,  # Regularization to prevent overfitting\n","    logging_dir=\"./logs\",  # Directory for logging training metrics\n","    push_to_hub=True,  # Upload to Hugging Face Hub\n","    hub_model_id=\"Mohan-DS-1321/Fine-Tuning-of-BERT\",  # Replace with your username and model name\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"muTLnQjIQ-g4","executionInfo":{"status":"ok","timestamp":1743159229786,"user_tz":-330,"elapsed":52,"user":{"displayName":"Mohan DS","userId":"05181729263002066506"}},"outputId":"2326cf77-2108-416b-ccd0-b512ec9fd475"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"]}]},{"cell_type":"markdown","source":["- `learning_rate=2e-5` â†’ A small learning rate prevents overfitting.\n","\n","- `num_train_epochs=3` â†’ Fine-tuning typically requires only a few epochs.\n","\n","- `per_device_train_batch_size=8` â†’ Defines batch size per GPU."],"metadata":{"id":"JdzRjUVmRcPH"}},{"cell_type":"markdown","source":["### Step 7: Train the Model\n"],"metadata":{"id":"MfBJy_M5Rkc3"}},{"cell_type":"code","source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n","    compute_metrics=compute_metrics,\n",")\n","\n","# Start fine-tuning\n","trainer.train()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":379},"id":"wCHgcFfuRiuI","executionInfo":{"status":"error","timestamp":1743160430621,"user_tz":-330,"elapsed":1200829,"user":{"displayName":"Mohan DS","userId":"05181729263002066506"}},"outputId":"14a7405d-d0eb-43bf-b1e5-9d29a6733a96"},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='87' max='12500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [   87/12500 19:30 < 47:29:24, 0.07 it/s, Epoch 0.01/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-24b81589eaee>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Start fine-tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2234\u001b[0m                 \u001b[0;31m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2236\u001b[0;31m                 return inner_training_loop(\n\u001b[0m\u001b[1;32m   2237\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2238\u001b[0m                     \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2554\u001b[0m                     )\n\u001b[1;32m   2555\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2556\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2558\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3762\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3764\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3766\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2357\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2359\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["- `Trainer` â†’ A high-level API that handles training and evaluation automatically.\n","\n","- `trainer.train()` â†’ Starts the fine-tuning process."],"metadata":{"id":"bX-tz8X8RooS"}},{"cell_type":"markdown","source":["### Step 8: Evaluate the Model"],"metadata":{"id":"C0ybWG-SRuYn"}},{"cell_type":"code","source":["# Evaluate the fine-tuned model on the test dataset\n","results = trainer.evaluate()\n","print(\"Evaluation Results:\", results)\n"],"metadata":{"id":"rlJ0mXs9RseY","executionInfo":{"status":"aborted","timestamp":1743160430627,"user_tz":-330,"elapsed":1,"user":{"displayName":"Mohan DS","userId":"05181729263002066506"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Upload Fine-Tuned Model to Hugging Face Hub"],"metadata":{"id":"HosLAuRzU-cJ"}},{"cell_type":"code","source":["# Push model to the Hugging Face Model Hub\n","trainer.push_to_hub()\n"],"metadata":{"id":"s30XcoBXU_P5","executionInfo":{"status":"aborted","timestamp":1743160430630,"user_tz":-330,"elapsed":1,"user":{"displayName":"Mohan DS","userId":"05181729263002066506"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 1ï¸âƒ£1ï¸âƒ£ Use the Fine-Tuned Model for Predictions\n","- After fine-tuning, you can load and use the model for inference:"],"metadata":{"id":"4wdTczLEVC63"}},{"cell_type":"code","source":["from transformers import pipeline\n","\n","# Load the fine-tuned model from Hugging Face\n","model_pipeline = pipeline(\"text-classification\", model=\"Mohan-DS-1321/Fine-Tuning-of-BERT\")\n","\n","# Test the model on a new review\n","review = \"This movie was absolutely amazing! I loved it.\"\n","result = model_pipeline(review)\n","\n","print(result)\n"],"metadata":{"id":"rdwh0cgIVBM1","executionInfo":{"status":"aborted","timestamp":1743160430633,"user_tz":-330,"elapsed":1,"user":{"displayName":"Mohan DS","userId":"05181729263002066506"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ðŸ“Œ Output Example:\n","# [{'label': 'LABEL_1', 'score': 0.98}]\n","\n","# LABEL_1 â†’ Positive review\n","\n","# LABEL_0 â†’ Negative review"],"metadata":{"id":"EGX1E-o4VGmx","executionInfo":{"status":"aborted","timestamp":1743160430637,"user_tz":-330,"elapsed":1,"user":{"displayName":"Mohan DS","userId":"05181729263002066506"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from huggingface_hub import HfApi\n","\n","api = HfApi()\n","models = api.list_models(author=\"Mohan-DS-1321\")  # Replace with your username\n","\n","for model in models:\n","    print(model.modelId)  # Prints all models you have uploaded\n"],"metadata":{"id":"WLl61pJqWPkc","executionInfo":{"status":"aborted","timestamp":1743160430640,"user_tz":-330,"elapsed":1,"user":{"displayName":"Mohan DS","userId":"05181729263002066506"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Summary of Steps\n","- Step         ---------------------------- Description\n","1. Install Packages\t:  Install transformers, datasets, torch, huggingface_hub\n","2. Import Libraries\t:  Load required modules\n","3. Authenticate with API Key:\t Log in using your Hugging Face token\n","4. Load Pretrained Model\t:  Load bert-base-uncased for text classification\n","5. Load Dataset\tLoad :  IMDB movie reviews dataset\n","6. Define Metrics :\t Accuracy, Precision, Recall, F1-score\n","7. Training Config :\t Set hyperparameters like batch size and learning rate\n","8. Fine-Tune Model :\t Train using Hugging Face Trainer\n","9. Evaluate Model :\t Check accuracy on test dataset\n","10. Upload to Hugging Face :\t trainer.push_to_hub() to share model\n","11. Use Model for Predictions : \tLoad and test it on new text"],"metadata":{"id":"cAGFg4ZdbWgk"}}]}